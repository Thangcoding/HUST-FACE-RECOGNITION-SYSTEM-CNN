{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvwzoTp-mLTQ"
      },
      "source": [
        "# Tranfer learning VGG16 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZMrwr-lmLTS"
      },
      "source": [
        "## Set up notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdeGab-7mLTT"
      },
      "source": [
        "### Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9JkpR4LGG74D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8ru4MdCG74F",
        "outputId": "6c8749d6-c5bd-40da-ec10-b3f38d19d248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJX-5iSxmLTW"
      },
      "source": [
        "### Limit GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ6fTsfMtPPU",
        "outputId": "e77309b6-d11e-4f83-be37-dc4e1ef6c797"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y84ZO55G74F"
      },
      "source": [
        "## Preparing data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-u4-gE9G74F"
      },
      "source": [
        "### Import data from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwJ3SajS7m8q",
        "outputId": "16640e1d-d5dd-46e1-f7b1-af51f96eb606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf7A8PTjMISX",
        "outputId": "8fd51609-c5f4-45f9-9882-bdace3257982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data exist: True\n",
            "test data exist: True\n"
          ]
        }
      ],
      "source": [
        "train_dir = Path(\"/content/drive/MyDrive/Newest_data_train\")\n",
        "test_dir = Path(\"/content/drive/MyDrive/Newest_data_test\")\n",
        "print(\"train data exist:\", train_dir.exists())\n",
        "print(\"test data exist:\", test_dir.exists())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nPOFhVVG74G",
        "outputId": "7412bafb-2355-460c-8178-36b4091fb9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3147 images found\n"
          ]
        }
      ],
      "source": [
        "train_img_count = len(list(train_dir.glob(\"*/*\")))\n",
        "test_img_count = len(list(test_dir.glob(\"*/*\")))\n",
        "print(train_img_count + test_img_count, \"images found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3y_7crYryTu"
      },
      "source": [
        "### Load data into training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URdGI8bkG74H",
        "outputId": "4646a454-0612-4884-abbe-893f90e038a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2481 files belonging to 99 classes.\n",
            "Found 657 files belonging to 99 classes.\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = (224, 224)\n",
        "COLOR = \"rgb\"\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    shuffle=True,\n",
        "    seed=716,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    color_mode=COLOR,\n",
        ")\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=test_dir,\n",
        "    shuffle=True,\n",
        "    seed=295,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    color_mode=COLOR,\n",
        ")\n",
        "CLASS_NAMES = train_ds.class_names\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "NUM_BATCH = int(train_ds.cardinality().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUvPTv9qyEyd",
        "outputId": "5d11de65-c36e-4223-d7e1-fc38d58a9419"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Alex Lawther',\n",
              " 'Amber Heart',\n",
              " 'Angelia Jolie',\n",
              " 'Avicii',\n",
              " 'Ben Affleck',\n",
              " 'Benzema',\n",
              " 'Bill Gates',\n",
              " 'Brie Larson',\n",
              " 'Calvin Harris',\n",
              " 'Chipu-error',\n",
              " 'Chris Evans',\n",
              " 'Chris Martin',\n",
              " 'Chris Pratt',\n",
              " 'Cillian Murphy',\n",
              " 'Claudia Salas',\n",
              " 'Cristiana Ronaldo',\n",
              " 'Dakota Johnson',\n",
              " 'Dat Ta',\n",
              " 'David Beckham',\n",
              " 'Doug McMillion',\n",
              " 'Drake-error',\n",
              " 'Duy Dat',\n",
              " 'Elizabeth Olsen',\n",
              " 'Elle Fanning',\n",
              " 'Elon Musk',\n",
              " 'Emilia Clarke',\n",
              " 'Emily Blunt',\n",
              " 'Gal Gadot',\n",
              " 'Gordon Ramsey',\n",
              " 'Hanni NewJeans',\n",
              " 'Henry Cavil',\n",
              " 'Ho Ngoc Ha',\n",
              " 'Irene',\n",
              " 'J.K.Rowling',\n",
              " 'Jack Ma',\n",
              " 'Jackie Chan',\n",
              " 'Jason Momoa',\n",
              " 'Jeff Bezos',\n",
              " 'Jenna Ortega',\n",
              " 'Jennie Blackpink',\n",
              " 'Jennifer Lawrence',\n",
              " 'Jessica Barden',\n",
              " 'Jisoo-it',\n",
              " 'Joe Biden',\n",
              " 'Karen Gillan',\n",
              " 'Keanu Reeves',\n",
              " 'Kit Harington',\n",
              " 'LeBron James-it',\n",
              " 'Leonardo Dicaprio',\n",
              " 'Lewis Hamilton-it',\n",
              " 'Lily Collins',\n",
              " 'Lisa-it',\n",
              " 'Marc Marquez-it',\n",
              " 'Marie Curie-it',\n",
              " 'Mark Wahlberg',\n",
              " 'Megan Fox',\n",
              " 'Messi',\n",
              " 'Micheal B Jordan',\n",
              " 'Mina Twice',\n",
              " 'Mohamed Salah',\n",
              " 'Nancy Momoland',\n",
              " 'Nick Vujicic',\n",
              " 'Olivia Rodrigo-it',\n",
              " 'Oprah Winfrey',\n",
              " 'Ozil',\n",
              " 'Quan Nguyen',\n",
              " 'Quynh Nguyen',\n",
              " 'Robert Pattinson',\n",
              " 'Rose Blackpink',\n",
              " 'Rose Leslie',\n",
              " 'Sam Claflin',\n",
              " 'Shakira Face',\n",
              " 'Sophie Turner',\n",
              " 'Steven Spielberge',\n",
              " 'Suzy Bae',\n",
              " 'Taylor Swift',\n",
              " 'Thang Doi',\n",
              " 'Tom Cruise',\n",
              " 'Tom Holland',\n",
              " 'Tzuyu',\n",
              " 'Ursula Corbero',\n",
              " 'Vladimir V.Putin',\n",
              " 'Willem Dafoe',\n",
              " 'Yoona',\n",
              " 'Yuna Itzy',\n",
              " 'Zedd',\n",
              " 'cong phuong',\n",
              " 'do my linh',\n",
              " 'khanh vy',\n",
              " 'luu diec phi',\n",
              " 'mai phuong thuy',\n",
              " 'my tam',\n",
              " 'ngo bao chau-it',\n",
              " 'nguyen phu trong',\n",
              " 'park hang seo face',\n",
              " 'park seo joon face',\n",
              " 'pham nhat vuong',\n",
              " 'tran thanh',\n",
              " 'vu duc dam']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "CLASS_NAMES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC4cCe97G74J"
      },
      "source": [
        "### Visualize the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVnK4eOLG74L"
      },
      "source": [
        "### Data augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PEeqAGSwG74L"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import RandomBrightness, RandomFlip\n",
        "from keras.layers import RandomRotation, RandomZoom\n",
        "\n",
        "data_augmentation = Sequential(\n",
        "    [\n",
        "        RandomBrightness(0.3),\n",
        "        RandomFlip(\"horizontal\"),\n",
        "        RandomRotation(0.2),\n",
        "        RandomZoom(0.3),\n",
        "    ],\n",
        "    name=\"augmentation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs-fl2ZM_bMj"
      },
      "source": [
        "### Configure dataset for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiFXGrw_P6jj"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG9dWg7aH9jM"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvShI5Md8LE7"
      },
      "source": [
        "### Import VGG16 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHulVWHH8QAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827328f2-fe36-4a15-e952-b0f3a2c36549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "vgg16_model = VGG16(weights=\"imagenet\",\n",
        "                   include_top=False,\n",
        "                   input_shape=IMAGE_SIZE + (3,),\n",
        "                   )\n",
        "vgg16_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXuvz0Fy_0Cc",
        "outputId": "7a538411-acaa-4088-9d5d-594ebc8b315b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vgg16_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R_U-sFo8TqB"
      },
      "source": [
        "### Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2YeJ4ziQs7q"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Dense\n",
        "from keras.layers import Rescaling, Flatten, Dropout, BatchNormalization\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential(name=\"VGG16\",\n",
        "        layers = [\n",
        "            Input(shape=IMAGE_SIZE + (3,), name=\"input\"),\n",
        "            # Preprocess\n",
        "            data_augmentation,\n",
        "            Rescaling(1.0 / 255),\n",
        "            # Transfer learning\n",
        "            vgg16_model,\n",
        "            # Flatten and FC\n",
        "            Flatten(),\n",
        "            Dense(512, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.6),\n",
        "            Dense(512, activation = \"relu\"),\n",
        "            BatchNormalization(),\n",
        "            Dense(NUM_CLASSES, activation=\"softmax\", name=\"output\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Create a basic model instance\n",
        "model = create_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWKiGN6DRBPc"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e95QEg5mxiDc",
        "outputId": "8c64ef29-f493-4fcd-ee54-0b4fdb8fb657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"VGG16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " augmentation (Sequential)   (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               12845568  \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 512)               2048      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 512)               2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " output (Dense)              (None, 99)                50787     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27877795 (106.35 MB)\n",
            "Trainable params: 13161059 (50.21 MB)\n",
            "Non-trainable params: 14716736 (56.14 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_path = \"/content/drive/MyDrive/Model/vgg16.keras\"\n",
        "weights_path = \"/content/drive/MyDrive/Model/vgg16_weights.keras\"\n",
        "model.build()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XYo0VtWQ-hO"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8SycZsWG74M",
        "outputId": "fb65e725-d01f-400d-a5e3-e0d3db3bddb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "78/78 [==============================] - 612s 7s/step - loss: 4.5918 - accuracy: 0.0645 - val_loss: 6.0285 - val_accuracy: 0.0441\n",
            "Epoch 2/200\n",
            "78/78 [==============================] - 14s 176ms/step - loss: 3.6554 - accuracy: 0.1640 - val_loss: 3.5616 - val_accuracy: 0.1887\n",
            "Epoch 3/200\n",
            "78/78 [==============================] - 16s 208ms/step - loss: 3.1512 - accuracy: 0.2213 - val_loss: 3.4750 - val_accuracy: 0.2496\n",
            "Epoch 4/200\n",
            "78/78 [==============================] - 14s 174ms/step - loss: 2.7557 - accuracy: 0.3128 - val_loss: 2.3086 - val_accuracy: 0.3881\n",
            "Epoch 5/200\n",
            "78/78 [==============================] - 13s 170ms/step - loss: 2.5164 - accuracy: 0.3482 - val_loss: 1.9936 - val_accuracy: 0.4505\n",
            "Epoch 6/200\n",
            "78/78 [==============================] - 13s 166ms/step - loss: 2.3684 - accuracy: 0.3716 - val_loss: 2.0316 - val_accuracy: 0.4505\n",
            "Epoch 7/200\n",
            "78/78 [==============================] - 13s 173ms/step - loss: 2.1806 - accuracy: 0.4079 - val_loss: 1.9672 - val_accuracy: 0.4673\n",
            "Epoch 8/200\n",
            "78/78 [==============================] - 14s 174ms/step - loss: 2.1195 - accuracy: 0.4281 - val_loss: 1.8736 - val_accuracy: 0.4840\n",
            "Epoch 9/200\n",
            "78/78 [==============================] - 14s 178ms/step - loss: 1.9988 - accuracy: 0.4377 - val_loss: 1.8789 - val_accuracy: 0.4901\n",
            "Epoch 10/200\n",
            "77/78 [============================>.] - ETA: 0s - loss: 1.9205 - accuracy: 0.4643\n",
            "Epoch 10: saving model to /content/drive/MyDrive/Model/vgg16.keras\n",
            "78/78 [==============================] - 19s 246ms/step - loss: 1.9169 - accuracy: 0.4651 - val_loss: 1.8584 - val_accuracy: 0.4962\n",
            "Epoch 11/200\n",
            "78/78 [==============================] - 13s 170ms/step - loss: 1.8761 - accuracy: 0.4744 - val_loss: 1.6118 - val_accuracy: 0.5571\n",
            "Epoch 12/200\n",
            "78/78 [==============================] - 13s 171ms/step - loss: 1.7935 - accuracy: 0.5103 - val_loss: 1.6127 - val_accuracy: 0.5403\n",
            "Epoch 13/200\n",
            "78/78 [==============================] - 14s 178ms/step - loss: 1.6697 - accuracy: 0.5320 - val_loss: 1.5979 - val_accuracy: 0.5495\n",
            "Epoch 14/200\n",
            "78/78 [==============================] - 13s 172ms/step - loss: 1.6751 - accuracy: 0.5240 - val_loss: 1.8225 - val_accuracy: 0.5068\n",
            "Epoch 15/200\n",
            "78/78 [==============================] - 14s 176ms/step - loss: 1.6015 - accuracy: 0.5393 - val_loss: 1.5332 - val_accuracy: 0.5601\n",
            "Epoch 16/200\n",
            "78/78 [==============================] - 13s 170ms/step - loss: 1.5983 - accuracy: 0.5409 - val_loss: 1.4115 - val_accuracy: 0.6073\n",
            "Epoch 17/200\n",
            "78/78 [==============================] - 14s 175ms/step - loss: 1.5773 - accuracy: 0.5623 - val_loss: 1.3642 - val_accuracy: 0.5860\n",
            "Epoch 18/200\n",
            "30/78 [==========>...................] - ETA: 6s - loss: 1.5702 - accuracy: 0.5510"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "EPOCHS = 200\n",
        "FREQUENCY = 10\n",
        "\n",
        "def custom_lr(epoch):\n",
        "    if epoch < 50:\n",
        "        return 0.001\n",
        "    if epoch < 80:\n",
        "      return 0.0005\n",
        "    return 0.00001\n",
        "\n",
        "lr = LearningRateScheduler(custom_lr)\n",
        "\n",
        "mc = ModelCheckpoint(\n",
        "    filepath=model_path,\n",
        "    monitor=\"val_loss\",\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=FREQUENCY * NUM_BATCH,\n",
        ")\n",
        "\n",
        "es = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=40,\n",
        "    start_from_epoch=EPOCHS,\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    verbose=1,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[mc],\n",
        "    validation_data=val_ds,\n",
        ")\n",
        "with open(\"/content/drive/MyDrive/Model/vgg16.txt\", \"w\") as file:\n",
        "    for p, l in history.history.items():\n",
        "        file.write(str(p) + \": \" + str(l) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3NVtYu-mLTi"
      },
      "source": [
        "### Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD7zcObhyUA2"
      },
      "outputs": [],
      "source": [
        "# create history loss and accuracy function\n",
        "def plot_loss_acc(history):\n",
        "    train_loss = history.history[\"loss\"]\n",
        "    val_loss = history.history[\"val_loss\"]\n",
        "    acc = history.history[\"accuracy\"]\n",
        "    val_acc = history.history[\"val_accuracy\"]\n",
        "    epochs = range(len(train_loss))\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "    # Plot loss\n",
        "    ax[1].set_title(\"Model loss\")\n",
        "    ax[1].set_xlabel(\"Epochs\")\n",
        "    ax[1].plot(epochs, train_loss, color=\"b\", label=\"Train\")\n",
        "    ax[1].plot(epochs, val_loss, color=\"r\", label=\"Validation\")\n",
        "    fig.legend()\n",
        "    # Plot accuracy\n",
        "    ax[0].set_title(\"Model accuracy\")\n",
        "    ax[0].set_xlabel(\"Epochs\")\n",
        "    ax[0].plot(epochs, acc, color=\"b\", label=\"Train\")\n",
        "    ax[0].plot(epochs, val_acc, color=\"r\", label=\"Validation\")\n",
        "\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7fcAFU1yUwW"
      },
      "outputs": [],
      "source": [
        "plot_loss_acc(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtmy4n7tmLTj"
      },
      "source": [
        "### Save the model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RH2wQWVDE_5"
      },
      "outputs": [],
      "source": [
        "model.save_weights(weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kffITf5mLTk"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs_r72R1mLTk"
      },
      "source": [
        "### Load trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqUzQIWjNHzw"
      },
      "outputs": [],
      "source": [
        "trained_model = create_model()\n",
        "trained_model.load_weights(weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ombPxzUFmLTl"
      },
      "source": [
        "### Show prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWaZQWDLPhKB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "\n",
        "test_batch = val_ds.take(1)\n",
        "prediction = trained_model.predict(test_batch)\n",
        "for images, labels in test_batch:\n",
        "    for i in range(25):\n",
        "        img = images[i].numpy().astype(\"uint8\")\n",
        "        label = CLASS_NAMES[labels[i]]\n",
        "        predict_label = CLASS_NAMES[np.argmax(prediction[i])]\n",
        "        confidence = prediction[i][np.argmax(prediction[i])]\n",
        "\n",
        "        ax = plt.subplot(5, 5, i + 1)\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "        plt.title(f\"Label: {label}\\n Predict: {predict_label} {confidence :.2%}\",fontsize = 12)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "# plt.savefig(\"/content/drive/MyDrive/Model/result.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-3OM3gCG65m"
      },
      "outputs": [],
      "source": [
        "test_img = keras.utils.load_img(\n",
        "    \"/content/drive/MyDrive/Dataset/Newest_data_test/vu duc dam/vu duc dam10.jpg\",\n",
        "    # \"/content/drive/MyDrive/Model/410076200_1036268900763444_7352011443897460876_n.jpg\",\n",
        "    target_size=(224,224,3),\n",
        ")\n",
        "test_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k05o_NYkFl27"
      },
      "outputs": [],
      "source": [
        "pre = trained_model.predict(np.asarray(test_img)[None,:,:])\n",
        "print(CLASS_NAMES[np.argmax(pre)], pre[0][np.argmax(pre)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_IyANG7Qw_M"
      },
      "outputs": [],
      "source": [
        "# All test list\n",
        "with open(\"/content/drive/MyDrive/Model/res.txt\", \"w\") as file:\n",
        "    for dir in test_dir.iterdir():\n",
        "        print(dir)\n",
        "        file.write(dir.name + \"\\n\")\n",
        "        for img_path in dir.iterdir():\n",
        "            test_img = keras.utils.load_img(\n",
        "                img_path,\n",
        "                target_size=(224,224,3),\n",
        "                color_mode=\"grayscale\")\n",
        "            test_prediction = trained_model.predict(np.asarray(test_img)[None,:,:])\n",
        "            test_label = CLASS_NAMES[np.argmax(test_prediction)]\n",
        "            test_confidence = test_prediction[0][np.argmax(test_prediction)]\n",
        "            line = f\"{img_path.name} Pre: {test_label} {test_confidence :.2%}\"\n",
        "            print(line)\n",
        "            file.write(\"   \"+ line + \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}